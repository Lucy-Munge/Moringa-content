{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Scraping Concerts - Lab\n", "\n", "## Introduction\n", "\n", "Now that you've seen how to scrape a simple website, it's time to again practice those skills on a full-fledged site!\n", "\n", "In this lab, you'll practice your scraping skills on an online music magazine and events website called Resident Advisor.\n", "\n", "## Objectives\n", "\n", "You will be able to:\n", "\n", "* Create a full scraping pipeline that involves traversing over many pages of a website, dealing with errors and storing data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## View the Website\n", "\n", "For this lab, you'll be scraping the https://ra.co website. For reproducibility we will use the [Internet Archive](https://archive.org/) Wayback Machine to retrieve a version of this page from March 2019.\n", "\n", "Start by navigating to the events page [here](https://web.archive.org/web/20210325230938/https://ra.co/events/us/newyork?week=2019-03-30) in your browser. It should look something like this:\n", "\n", "<img src=\"images/ra_top.png\">"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Open the Inspect Element Feature\n", "\n", "Next, open the inspect element feature from your web browser in order to preview the underlying HTML associated with the page."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Write a Function to Scrape all of the Events on the Given Page\n", "\n", "The function should return a Pandas DataFrame with columns for the `Event_Name`, `Venue`, and `Number_of_Attendees`.\n", "\n", "Start by importing the relevant libraries, making a request to the relevant URL, and exploring the contents of the response with `BeautifulSoup`. Then fill in the `scrape_events` function with the relevant code."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Relevant imports"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["EVENTS_PAGE_URL = \"https://web.archive.org/web/20210326225933/https://ra.co/events/us/newyork?week=2019-03-30\"\n", "\n", "# Exploration: making the request and parsing the response\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Find the container with event listings in it\n", "# Some hints are giving along the way\n", "\n", "# This page is organized somewhat unusually, and many of\n", "# the CSS attributes seem auto-generated. We notice that\n", "# there is a div with \"events-all\" in its attributes that\n", "# looks promising if we use soup.find(), call this events_all_div\n", "\n", "events_all_div = None\n", "\n", "# The actual content is nested in a ul containing a single\n", "# li within that div. Unclear why they are using a \"list\"\n", "# concept for one element, but let's go ahead and select it\n", "# Call this event_listings and use it to find ul and li in \n", "# events_all_div\n", "\n", "event_listings = None\n", "\n", "\n", "# Print out some chunks of the text inside to make sure we\n", "# have everything we need in here\n", "# For example print the events for March 30th and 31st\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Find a list of events by date within that container\n", "\n", "# Now we look at what is inside of that event_listings li tag.\n", "# Based on looking at the HTML with developer tools, we see\n", "# that there are 13 children of that tag, all divs. Each div\n", "# is either a container of events on a given date, or empty\n", "\n", "# Let's create a collection of those divs. recursive=False\n", "# means we stop at 1 level below the event_listings li\n", "dates = event_listings.findChildren(recursive=False)\n", "\n", "# Now let's print out the start of the March 30th and March\n", "# 31st sections again. This time each is in its own \"date\"\n", "# container\n", "\n", "# March 30th is at the 0 index\n", "print(\"0 index:\", dates[0].text[:200])\n", "print()\n", "# The 1 index is empty. We'll need to skip this later\n", "print(\"1 index: \", dates[1].text)\n", "print()\n", "# March 31st is at the 2 index\n", "print(\"2 index:\", dates[2].text[:200])\n", "\n", "# Now we know we can loop over all of the items in the dates\n", "# list of divs to find the dates, although some will be blank\n", "# so we'll need to skip them"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Extract the date (e.g. Sat, 30 Mar) from one of those containers\n", "# Call this first_date\n", "\n", "# Grabbing just one to practice on\n", "first_date = None\n", "\n", "# This div contains a div with the date, followed by several uls\n", "# containing actual event information\n", "\n", "# The div with the date happens to have another human-readable\n", "# CSS class, so let's use that to select it then grab its text\n", "# Call this date, and use class_=sticky header as an argument for\n", "# first_date.find\n", "date = None\n", "\n", "# There is a / thing used for aesthetic reasons; let's remove it\n", "date = None"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Extract the name, venue, and number of attendees from one of the\n", "# events within that container\n", "\n", "# As noted previously, the div with information about events on\n", "# this date contains several ul tags, each with information about\n", "# a specific event. Get a list of them.\n", "# (Again this is an odd use of HTML, to have an unordered list\n", "# containing a single list item. But we scrape what we find!)\n", "first_date_events = None\n", "\n", "# Grabbing the first event ul to practice on\n", "first_event = None\n", "\n", "# Each event ul contains a single h3 with the event name, easy enough\n", "name = None\n", "\n", "# First, get all 1-3 divs that match this description,\n", "# where first_event.findAll has attrs={\"height\": 30}\n", "# as one of its arguments\n", "venue_and_attendees = None\n", "# The venue is the 0th (left-most) div, get its text\n", "venue = None\n", "# The number of attendees is the last div (although it's sometimes\n", "# missing), get its text\n", "num_attendees = None"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run the code below\n", "# Make sure you understand it since it will\n", "# for the basis of the definition of scrape_events below\n", "\n", "# Create an empty list to hold results\n", "rows = []\n", "\n", "# Loop over all date containers on the page\n", "for date_container in dates:\n", "    \n", "    # First check if this is one of the empty divs. If it is,\n", "    # skip ahead to the next one\n", "    if not date_container.text:\n", "        continue\n", "    \n", "    # Same logic as above to extract the date\n", "    date = date_container.find(\"div\", class_=\"sticky-header\").text\n", "    date = date.strip(\"'\u0338\")\n", "    \n", "    # This time, loop over all of the events\n", "    events = date_container.findChildren(\"ul\")\n", "    for event in events:\n", "        \n", "        # Same logic as above to extract the name, venue, attendees\n", "        name = event.find(\"h3\").text\n", "        venue_and_attendees = event.findAll(\"div\", attrs={\"height\": 30})\n", "        venue = venue_and_attendees[0].text\n", "        try:\n", "            num_attendees = int(venue_and_attendees[-1].text)\n", "        except ValueError:\n", "            num_attendees = np.nan\n", "            \n", "        # New piece here: appending the new information to rows list\n", "        rows.append([name, venue, date, num_attendees])\n", "\n", "# Make the list of lists into a dataframe and display\n", "df = pd.DataFrame(rows)\n", "df  "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Bring it all together in a function that makes the request, gets the\n", "# list of entries from the response, loops over that list to extract the\n", "# name, venue, date, and number of attendees for each event, and returns\n", "# that list of events as a dataframe\n", "\n", "def scrape_events(events_page_url):\n", "    #Your code here\n", "    df.columns = [\"Event_Name\", \"Venue\", \"Event_Date\", \"Number_of_Attendees\"]\n", "    return df"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Test out your function\n", "scrape_events(EVENTS_PAGE_URL)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Write a Function to Retrieve the URL for the Next Page\n", "\n", "As you scroll down, there should be a button labeled \"Next Week\" that will take you to the next page of events. Write code to find that button and extract the URL from it.\n", "\n", "This is a relative path, so make sure you add `https://web.archive.org` to the front to get the URL.\n", "\n", "![next page](images/ra_next.png)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Find the button, find the relative path, create the URL for the current `soup`\n", "\n", "# This is tricky again, since there are not a lot of\n", "# human-readable CSS classes\n", "\n", "# One unique thing we notice is a > icon on the part where\n", "# you click to go to the next page. It's an SVG with an \n", "# aria-label of \"Right arrow\", this soup.find() will have\n", "# attrs={\"aria-label\": \"Right arrow\"} as an argument\n", "\n", "avg = None\n", "\n", "# That SVG is inside of a div\n", "svg_parent = None\n", "\n", "# And the tag right before that div (its \"previous sibling\")\n", "# is an anchor (link) tag with the path we need\n", "link = None\n", "\n", "# Then we can extract the path from that link to build the full URL\n", "relative_path = None\n", "next_page_url = None\n", "next_page_url"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Fill in this function, to take in the current page's URL and return the\n", "# next page's URL\n", "def next_page(url):\n", "    #Your code here\n", "    return next_page_url"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Test out your function\n", "next_page(EVENTS_PAGE_URL)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Scrape the Next 500 Events\n", "\n", "In other words, repeatedly call `scrape_events` and `next_page` until you have assembled a dataframe with at least 500 rows.\n", "\n", "Display the data sorted by the number of attendees, greatest to least.\n", "\n", "We recommend adding a brief `time.sleep` call between `requests.get` calls to avoid rate limiting."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your code here\n", "\n", "# Make a dataframe to store results. We will concatenate\n", "# additional dfs as they are returned\n", "overall_df = pd.DataFrame()\n", "\n", "current_url = EVENTS_PAGE_URL\n", "\n", "# Now define a while look on overall_df"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Display overall_df the specified sorted order\n", "# Do so by Number of Attendees in descending order"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Summary \n", "\n", "Congratulations! In this lab, you successfully developed a pipeline to scrape a website for concert event information!"]}], "metadata": {"kernelspec": {"display_name": "Python (learn-env)", "language": "python", "name": "learn-env"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 2}