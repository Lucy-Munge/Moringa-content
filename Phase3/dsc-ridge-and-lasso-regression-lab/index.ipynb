{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Ridge and Lasso Regression - Lab"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Introduction"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this lab, you'll practice your knowledge of ridge and lasso regression!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Objectives"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this lab you will: \n", "\n", "- Use lasso and ridge regression with scikit-learn \n", "- Compare and contrast lasso, ridge and non-regularized regression "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Housing Prices Data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We'll use this version of the Ames Housing dataset: "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "import pandas as pd\n", "import warnings\n", "warnings.filterwarnings('ignore')\n", "df = pd.read_csv('housing_prices.csv', index_col=0)\n", "df.info()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["More information about the features is available in the `data_description.txt` file in this repository."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Data Preparation\n", "\n", "The code below:\n", "\n", "* Separates the data into `X` (predictor) and `y` (target) variables\n", "* Splits the data into 75-25 training-test sets, with a `random_state` of 10\n", "* Separates each of the `X` values into continuous vs. categorical features\n", "* Fills in missing values (using different strategies for continuous vs. categorical features)\n", "* Scales continuous features to a range of 0 to 1\n", "* Dummy encodes categorical features\n", "* Combines the preprocessed continuous and categorical features back together"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "import numpy as np\n", "from sklearn.impute import SimpleImputer\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n", "\n", "# Create X and y\n", "y = df['SalePrice']\n", "X = df.drop(columns=['SalePrice'])\n", "\n", "# Split data into training and test sets\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10)\n", "\n", "# Separate X data into continuous vs. categorical\n", "X_train_cont = X_train.select_dtypes(include='number')\n", "X_test_cont = X_test.select_dtypes(include='number')\n", "X_train_cat = X_train.select_dtypes(exclude='number')\n", "X_test_cat = X_test.select_dtypes(exclude='number')\n", "\n", "# Impute missing values using SimpleImputer, median for continuous and\n", "# filling in 'missing' for categorical\n", "impute_cont = SimpleImputer(strategy='median')\n", "X_train_cont = impute_cont.fit_transform(X_train_cont)\n", "X_test_cont = impute_cont.transform(X_test_cont)\n", "impute_cat = SimpleImputer(strategy='constant', fill_value='missing')\n", "X_train_cat = impute_cat.fit_transform(X_train_cat)\n", "X_test_cat = impute_cat.transform(X_test_cat)\n", "\n", "# Scale continuous values using MinMaxScaler\n", "scaler = MinMaxScaler()\n", "X_train_cont = scaler.fit_transform(X_train_cont)\n", "X_test_cont = scaler.transform(X_test_cont)\n", "\n", "# Dummy encode categorical values using OneHotEncoder\n", "ohe = OneHotEncoder(handle_unknown='ignore')\n", "X_train_cat = ohe.fit_transform(X_train_cat)\n", "X_test_cat = ohe.transform(X_test_cat)\n", "\n", "# Combine everything back together\n", "X_train_preprocessed = np.concatenate([X_train_cont, X_train_cat.todense()], axis=1)\n", "X_test_preprocessed = np.concatenate([X_test_cont, X_test_cat.todense()], axis=1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Linear Regression Model\n", "\n", "Let's use this data to build a first naive linear regression model. Fit the model on the training data (`X_train_preprocessed`), then compute the R-Squared and the MSE for both the training and test sets."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "from sklearn.metrics import mean_squared_error\n", "from sklearn.linear_model import LinearRegression\n", "\n", "# Fit the model\n", "linreg = None\n", "\n", "# Print R2 and MSE for training and test sets\n", "None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Notice the severe overfitting above; our training R-Squared is very high, but the test R-Squared is negative! Similarly, the scale of the test MSE is orders of magnitude higher than that of the training MSE."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Ridge and Lasso Regression"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Use all the data (scaled features and dummy categorical variables, `X_train_preprocessed`) to build some models with regularization - two each for lasso and ridge regression. Each time, look at R-Squared and MSE.\n", "\n", "Remember that you can use the scikit-learn documentation if you don't remember how to import or use these classes:\n", "\n", "* [`Lasso` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n", "* [`Ridge` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Lasso"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### With default hyperparameters (`alpha` = 1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your code here"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### With a higher regularization hyperparameter (`alpha` = 10)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your code here"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Ridge"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### With default hyperparameters (`alpha` = 1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your code here"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### With higher regularization hyperparameter (`alpha` = 10)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your code here"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Comparing the Metrics    \n", "\n", "Which model seems best, based on the metrics?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Write your conclusions here:\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<details>\n", "    <summary style=\"cursor: pointer\"><b>Answer (click to reveal)</b></summary>\n", "\n", "In terms of both R-Squared and MSE, the `Lasso` model with `alpha`=10 has the best metric results.\n", "\n", "(Remember that better R-Squared is higher, whereas better MSE is lower.)\n", "\n", "</details>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Comparing the Parameters\n", "\n", "Compare the number of parameter estimates that are (very close to) 0 for the `Ridge` and `Lasso` models with `alpha`=10.\n", "\n", "Use 10**(-10) as an estimate that is very close to 0. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Number of Ridge params almost zero\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Number of Lasso params almost zero\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Compare and interpret these results\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<details>\n", "    <summary style=\"cursor: pointer\"><b>Answer (click to reveal)</b></summary>\n", "\n", "The ridge model did not penalize any coefficients to 0, while the lasso model removed about 1/4 of the coefficients. The lasso model essentially performed variable selection for us, and got the best metrics as a result!\n", "\n", "</details>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Finding an Optimal Alpha\n", "\n", "Earlier we tested two values of `alpha` to see how it affected our MSE and the value of our coefficients. We could continue to guess values of `alpha` for our ridge or lasso regression one at a time to see which values minimize our loss, or we can test a range of values and pick the alpha which minimizes our MSE. Here is an example of how we would do this:  "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "\n", "train_mse = []\n", "test_mse = []\n", "alphas = np.linspace(0, 200, num=50)\n", "\n", "for alpha in alphas:\n", "    lasso = Lasso(alpha=alpha)\n", "    lasso.fit(X_train_preprocessed, y_train)\n", "    \n", "    train_preds = lasso.predict(X_train_preprocessed)\n", "    train_mse.append(mean_squared_error(y_train, train_preds))\n", "    \n", "    test_preds = lasso.predict(X_test_preprocessed)\n", "    test_mse.append(mean_squared_error(y_test, test_preds))\n", "\n", "fig, ax = plt.subplots()\n", "ax.plot(alphas, train_mse, label='Train')\n", "ax.plot(alphas, test_mse, label='Test')\n", "ax.set_xlabel('alpha')\n", "ax.set_ylabel('MSE')\n", "\n", "# np.argmin() returns the index of the minimum value in a list\n", "optimal_alpha = alphas[np.argmin(test_mse)]\n", "\n", "# Add a vertical line where the test MSE is minimized\n", "ax.axvline(optimal_alpha, color='black', linestyle='--')\n", "ax.legend();\n", "\n", "print(f'Optimal Alpha Value: {int(optimal_alpha)}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Take a look at this graph of our training and test MSE against `alpha`. Try to explain to yourself why the shapes of the training and test curves are this way. Make sure to think about what `alpha` represents and how it relates to overfitting vs underfitting.\n", "\n", "---\n", "\n", "<details>\n", "    <summary style=\"cursor: pointer\"><b>Answer (click to reveal)</b></summary>\n", "\n", "For `alpha` values below 28, the model is overfitting. As `alpha` increases up to 28, the MSE for the training data increases and MSE for the test data decreases, indicating that we are reducing overfitting.\n", "\n", "For `alpha` values above 28, the model is starting to underfit. You can tell because _both_ the train and the test MSE values are increasing.\n", "\n", "</details>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Summary\n", "\n", "Well done! You now know how to build lasso and ridge regression models, use them for feature selection and find an optimal value for `alpha`. "]}], "metadata": {"kernelspec": {"display_name": "Python (learn-env)", "language": "python", "name": "learn-env"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 2}