{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Regression Trees and Model Optimization - Lab\n", "\n", "## Introduction\n", "\n", "In this lab, we'll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model. \n", "\n", "## Objectives\n", "\n", "In this lab you will: \n", "\n", "- Perform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models \n", "- Determine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Ames Housing dataset \n", "\n", "The dataset is available in the file `'ames.csv'`. \n", "\n", "- Import the dataset and examine its dimensions: "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Import necessary libraries\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "plt.style.use('ggplot')\n", "%matplotlib inline\n", "\n", "# Load the Ames housing dataset \n", "data = None\n", "\n", "# Print the dimensions of data\n", "\n", "\n", "# Check out the info for the dataframe\n", "\n", "\n", "# Show the first 5 rows\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Identify features and target data \n", "\n", "In this lab, we will use using 3 predictive continuous features:\n", "\n", "#### Features\n", "\n", "- `LotArea`: Lot size in square feet\n", "- `1stFlrSF`: Size of first floor in square feet\n", "- `GrLivArea`: Above grade (ground) living area square feet\n", "\n", "#### Target\n", "\n", "- `SalePrice`', the sale price of the home, in dollars"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- Create DataFrames for the features and the target variable as shown above \n", "- Inspect the contents of both the features and the target variable"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Features and target data\n", "target = None\n", "features = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Inspect correlations \n", "\n", "- Use scatter plots to show the correlation between the chosen features and the target variable\n", "- Comment on each scatter plot "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#\u00a0Your code here "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Create evaluation metrics\n", "\n", "- Import `r2_score` and `mean_squared_error` from `sklearn.metrics` \n", "- Create a function `performance(true, predicted)` to calculate and return both the R-squared score and Root Mean Squared Error (RMSE) for two equal-sized arrays for the given true and predicted values \n", "    - Depending on your version of sklearn, in order to get the RMSE score you will need to either set `squared=False` or you will need to take the square root of the output of the `mean_squared_error` function - check out [the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) or this helpful and related [StackOverflow post](https://stackoverflow.com/questions/17197492/is-there-a-library-function-for-root-mean-square-error-rmse-in-python)\n", "    - The benefit of calculating RMSE instead of the Mean Squared Error (MSE) is that RMSE is in the same units at the target - here, this means that RMSE will be in dollars, calculating how far off in dollars our predictions are away from the actual prices for homes, on average"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#\u00a0Import metrics\n", "\n", "\n", "# Define the function\n", "def performance(y_true, y_predict):\n", "    \"\"\" \n", "    Calculates and returns the two performance scores between \n", "    true and predicted values - first R-Squared, then RMSE\n", "    \"\"\"\n", "\n", "    # Calculate the r2 score between 'y_true' and 'y_predict'\n", "\n", "    # Calculate the root mean squared error between 'y_true' and 'y_predict'\n", "\n", "    # Return the score\n", "\n", "    pass\n", "\n", "\n", "# Test the function\n", "score = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\n", "score\n", "\n", "# [0.9228556485355649, 0.6870225614927066]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Split the data into training and test sets\n", "\n", "- Split `features` and `target` datasets into training/test data (80/20) \n", "- For reproducibility, use `random_state=42`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split \n", "\n", "# Split the data into training and test subsets\n", "x_train, x_test, y_train, y_test = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Grow a vanilla regression tree\n", "\n", "- Import the `DecisionTreeRegressor` class\n", "- Run a baseline model for later comparison using the datasets created above\n", "- Generate predictions for test dataset and calculate the performance measures using the function created above \n", "- Use `random_state=45` for tree instance\n", "- Record your observations"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#\u00a0Import DecisionTreeRegressor\n", "\n", "\n", "# Instantiate DecisionTreeRegressor \n", "# Set random_state=45\n", "regressor = None\n", "\n", "# Fit the model to training data\n", "\n", "\n", "# Make predictions on the test data\n", "y_pred = None\n", "\n", "# Calculate performance using the performance() function \n", "score = None\n", "score\n", "\n", "# [0.5961521990414137, 55656.48543887347] - R2, RMSE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Hyperparameter tuning (I)\n", "\n", "- Find the best tree depth using depth range: 1-30\n", "- Run the regressor repeatedly in a `for` loop for each depth value  \n", "- Use `random_state=45` for reproducibility\n", "- Calculate RMSE and r-squared for each run \n", "- Plot both performance measures for all runs \n", "- Comment on the output "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#\u00a0Your code here "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Hyperparameter tuning (II)\n", "\n", "- Repeat the above process for `min_samples_split` \n", "- Use a range of values from 2-10 for this hyperparameter \n", "- Use `random_state=45` for reproducibility\n", "- Visualize the output and comment on results as above "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#\u00a0Your code here "]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Run the *optimized* model \n", "\n", "- Use the best values for `max_depth` and `min_samples_split` found in previous runs and run an optimized model with these values \n", "- Calculate the performance and comment on the output "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#\u00a0Your code here "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Level up (Optional)\n", "\n", "- How about bringing in some more features from the original dataset which may be good predictors?\n", "- Also, try tuning more hyperparameters like `max_features` to find a more optimal version of the model "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#\u00a0Your code here "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Summary \n", "\n", "In this lab, we looked at applying a decision-tree-based regression analysis on the Ames Housing dataset. We saw how to train various models to find the optimal values for hyperparameters. "]}], "metadata": {"kernelspec": {"display_name": "Python (learn-env)", "language": "python", "name": "learn-env"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 2}