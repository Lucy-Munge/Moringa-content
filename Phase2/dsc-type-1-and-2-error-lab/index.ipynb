{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Type I and Type II Errors - Lab\n", "\n", "## Introduction\n", "\n", "In this lab, you'll run some of your own simulations to learn more about type I and type II errors. Remember that, the result of a statistical hypothesis test and the corresponding decision of whether to reject or accept the null hypothesis, is not infallible. A test provides evidence for or against the null hypothesis and then you decide whether to accept or reject it based on that evidence, but the evidence may lack the strength to arrive at the correct conclusion. Incorrect conclusions made from hypothesis tests fall in one of two categories, i.e. [Type I and Type II errors](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors). By running some of these simulations, you should have a better idea of why a 95% confidence level is often used for hypothesis testing.\n", "\n", "\n", "## Objectives\n", "\n", "You will be able to:\n", "\n", "* Differentiate how Type I and Type II errors relate to the p and z-value\n", "* Describe the relationship between alpha and Type I errors\n", "* Create simulations and visualizations to represent scenarios involving Type I and Type II errors\n", "\n", "## Alpha and Beta\n", "\n", "**Alpha ($\\alpha$):** is the probability of a Type I error i.e. finding a difference when a difference does not exist. \n", "\n", "Most medical literature uses an alpha cut-off of 5% (0.05), indicating a 5% chance that a significant difference is actually due to chance and is not a true difference. \n", "\n", "**Beta ($\\beta$):** is the probability of a Type II error i.e. not detecting a difference when one actually exists. \n", "\n", "Beta is directly related to study power (Power = $1 - \\beta$) which you will investigate further in the next lesson. Most medical literature uses a beta cut-off of 20% (0.2), indicating a 20% chance that a significant difference is missed. \n", "\n", "\n", "\n", "Now you will attempt to create a simulation to visualize this phenomenon using Python."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import scipy.stats as stats\n", "import matplotlib.pyplot as plt\n", "import math\n", "import random \n", "\n", "import seaborn as sns\n", "sns.set(color_codes=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": [" First, create a population of 1000 elements with a mean of 100 and a standard deviation of 20."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create a population with mean=100 and sd=20 and size = 1000\n", "pop = np.random.normal(100, 20, 1000)\n", "pop.dtype\n", "sns.distplot(pop)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now take two samples from this population and comment on the difference between their means and standard deviations. How would you ensure the independence between the elements of these samples? "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["k = 100\n", "sample1 = np.random.choice(pop,100,replace=True)\n", "\n", "print (\"Sample 1 Summary\")\n", "stats.describe(sample1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sample2 = np.random.choice(pop,100,replace=True)\n", "print (\"Sample 2 Summary\")\n", "stats.describe(sample2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can see that if you took two samples from this population, the difference between the mean of samples 1 and 2 is very small (this can be tried repeatedly). You must sample with replacement in order to ensure the independence assumption between elements of the sample. \n", "\n", "There is, however, still a probability of seeing a very large difference between values, even though they're estimates of the same population parameters. In a statistical setting, you'd interpret these unusually large differences as evidence that the two samples are statistically different. It depends on how you define statistical significance. In statistical tests, this is done by setting a significance threshold $\\alpha$  (alpha). Alpha controls how often we'll get a type I error. A type I error occurs when the statistical test erroneously indicates a significant result.\n", "\n", "You can run a two-sample t-test with the independence assumption on these samples and, as expected, the null hypothesis will fail to be rejected due to similarities between distributions. You can also visualize the distribution to confirm the similarity between means and SDs. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# test the sample means\n", "stats.ttest_ind(sample1, sample2)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(\"Test Samples\")\n", "sns.distplot(sample1, label='Sample1') \n", "sns.distplot(sample2, label='Sample2')\n", "plt.legend()\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Simulating Type I and II errors\n", "\n", "### Type I error\n", "Remember that when a hypothesis test is being performed, scientists are trying to determine if two samples are from the same population or not. When a hypothesis is rejected, they are concluding that a sample must have come from a different population. Type I error describes a situation where you reject the null hypothesis when it is actually true. It assumes two samples come from a _different_ population when, in reality, they are from the _same_ population. This type of error is also known as a \"false positive\" or \"false hit\". The type I error rate is equal to the significance level $\\alpha$, so setting a higher confidence level (and therefore lower $\\alpha$) reduces the chances of getting a false positive. \n", "\n", "\n", "\n", "### How alpha affects the prevalence of Type I errors.\n", "\n", "Next, we shall see how alpha affects the rate of type I errors. \n", "\n", "**Exercise:** Write a function `type_1_error` in Python to encapsulate the code shown above in order to repeat hypothesis tests on two randomly drawn distributions. The t-test will mostly fail to reject the null hypothesis, except, when by random chance you get a set of **extremely** different samples thus reject the null hypothesis (type I error). The frequency of such bad results depends upon the value of alpha. \n", "\n", "`type_1_error` should take in the parameters:\n", "\n", "* `population`: (NumPy array) a random normal distribution\n", "* `num_tests`: (int) specifies the number of hypothesis tests to compute\n", "* `alphas`: (list) a list of the alpha levels at which you are testing\n", "\n", "`type_1_error` should return:\n", "\n", "* `sig_tests`: (DataFrame) a dataframe that has the columns 'type_1_error', 'p_value', 'alpha'\n", "\n", "Within `type_1_error`, you should:\n", "\n", "1. Repeatedly take two random samples from `population` and run independent t-tests.    \n", "2. Store the p-value, alpha, and a boolean variable to show whether the null hypothesis **was rejected** or not (i.e. if p-value is less than alpha), for each test\n", "\n", "To test your function:\n", "\n", "1. Create a population distribution with a mean of 100, a standard deviation of 20, and a size of 1000 \n", "2. Specify the number of hypothesis tests to be 1000\n", "3. Create a list of alphas =  [0.001, 0.01, 0.05, 0.1, 0.2, 0.5]\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def type_1_error(population, num_tests, alpha_set):\n", "    \"\"\"\n", "    Parameters\n", "    ----------\n", "    population: ndarray\n", "        A random normal distribution\n", "    num_tests: int\n", "        The number of hypothesis tests to be computed\n", "    alpha_set: list\n", "        List of alpha levels\n", "    \n", "    Returns\n", "    ----------\n", "    sig_tests : DataFrame\n", "        A dataframe containing the columns 'type_1_error', 'p_value', and 'alpha'\n", "    \"\"\"\n", "    pass\n", "# Example dataframe for 1 test below"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we have to summarize the results, this is done using the pandas `groupby()` method which sums the `type_1_error` column for each level of alpha. The `groupby()` method iterates over each value of alpha, selecting the type I error column for all rows with a specific level of alpha, and then applies the sum function to the selection. \n", "\n", "What's the relationship between alpha and type I errors?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["# group type I error by values of alpha\n", "pop = None\n", "num_tests = None\n", "alpha_set = None\n", "sig_tests_1 = type_1_error(pop, num_tests, alpha_set)\n", "group_error = sig_tests_1.groupby('alpha')['type_1_error'].sum()\n", "group_error.plot.bar(title = \"TYPE I ERROR - FALSE POSITIVES\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The grouped data clearly shows that as value of alpha is increases from .001 to .5, the probability of type I errors also increases. \n", "\n", "##\u00a0Type II error \n", "\n", "This error describes a situation where you fail to reject the null hypothesis when it is actually false. Type II error is also known as a \"false negative\" or \"miss\". The higher your confidence level, the more likely you are to make a type II error.\n", "\n", "## How alpha affects the prevalence of Type II errors.\n", "\n", "**Exercise:** Write a function called `type_2_error` similar to the above except samples should be taken from two different populations.  The hypothesis test should, in most cases, reject the null hypothesis as the samples belong to different populations, except, in extreme cases where there is no significant difference between samples i.e. a type II error (False Negatives). Your function should demonstrate how the rate of false negatives is affected by alpha. \n", "\n", "`type_2_error` should take in the parameters:\n", "\n", "* `population`: (NumPy array) a random normal distribution\n", "* `population_2`: (NumPy array) a random normal distribution with a different mean than the population\n", "* `num_tests`: (int) specifies the number of hypothesis tests to compute\n", "* `alphas`: (list) a list of the alpha levels at which you are testing\n", "\n", "`type_2_error` should return:\n", "\n", "* `sig_tests`: (DataFrame) a dataframe that has the columns 'type_2_error', 'p_value', 'alpha'\n", "\n", "Within `type_2_error`, you should:\n", "\n", "1. Repeatedly take two random samples from population and run independent t-tests.    \n", "2. Store p_value, alpha, and a boolean variable to show whether the null hypothesis **failed to be rejected** or not (i.e. if p-value is less than alpha), for each test\n", "\n", "To test your function:\n", "\n", "1. Create a population distribution with a mean of 100, a standard deviation of 20, and a size of 1000 \n", "2. Create a second population distribution with a mean of 110, a standard deviation of 20, and a size of 1000\n", "3. Specify the number of hypothesis tests to be 1000\n", "4. Create a list of alphas =  [0.001, 0.01, 0.05, 0.1, 0.2, 0.5]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def type_2_error(population, population_2, num_tests, alpha_set):\n", "    \n", "    \"\"\"\n", "    Parameters\n", "    ----------\n", "    population: ndarray\n", "        A random normal distribution\n", "    population_2: ndarray\n", "        A different random normal distribution\n", "    num_tests: int\n", "        The number of hypothesis tests to be computed\n", "    alpha_set: list\n", "        List of alpha levels\n", "    \n", "    Returns\n", "    ----------\n", "    sig_tests : DataFrame\n", "        A dataframe containing the columns 'type_2_error', 'p_value', and 'alpha'\n", "    \"\"\"\n", "    pass\n", "# Example dataframe for 1 test below"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, create a visualization that will represent each one of these decisions. What's the relationship between alpha and type II errors?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pop = None\n", "pop2 = None\n", "num_tests = None\n", "alpha_set = None\n", "sig_tests_2 = type_2_error(pop,pop2,num_tests,alpha_set)\n", "\n", "group_error2 = sig_tests_2.groupby('alpha')['type_2_error'].sum()\n", "group_error2.plot.bar(title = \"Type II ERROR - FALSE NEGATIVES\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The grouped data clearly shows that as value of alpha is increased from .001 to .5, the probability of type II errors decreases. \n", "\n", "### Why is an \u03b1 level of 0.05 chosen as a cut-off for statistical significance?\n", "\n", "The $\\alpha$ level of 0.05 is considered s good balance to avoid excessive type I or type II errors. \n", "\n", "\n", "If you decide to use a large value for alpha : \n", "\n", "* Increases the chance of rejecting the null hypothesis\n", "* The risk of a type II error (false negative) is REDUCED\n", "* Risk of a type I error (false positive) is INCREASED\n", "\n", "Similarly, if you decide to use a very small value of alpha, it'll change the outcome as:\n", "* Increases the chance of accepting the null hypothesis\n", "* The risk of a Type I error (false positive) is REDUCED\n", "* Risk of a Type II error (false negative) is INCREASED\n", "\n", "From above, you can see that in statistical hypothesis testing, the more you try and avoid a type I error (false positive), the more likely a type II error (false negative) will occur. \n", "\n", "## Summary\n", "\n", "The key statistical point here is that there is always a trade off between false positives and false negatives. By increasing alpha, the number of false positives increases, but the number of false negatives decreases as shown in the bar graphs. The value of $\\alpha$ = 0.05 is considered a reasonable compromise between these two types of errors. Within the concept of \"significance,\" there is embedded a trade-off between these two types of errors. \n", "\n", "Think of \"significance\" as a compromise between false positives and negatives, not as absolute determination."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.3"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": true, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {}, "toc_section_display": true, "toc_window_display": false}}, "nbformat": 4, "nbformat_minor": 2}